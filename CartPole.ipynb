{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the modules And the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Laptop GPU'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "device_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(\"CartPole-v1\", render_mode='human')\n",
    "eval_env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  4\n",
      "Sample observation [ 4.6879525e+00  1.6285270e+38  3.7221727e-01 -1.3018692e+38]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "The Action Space is:  2\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size) -> None:\n",
    "        super().__init__() # calling the super class\n",
    "        \n",
    "        #Network layers \n",
    "        self.L1 = nn.Linear(s_size, h_size)\n",
    "        self.L2 = nn.Linear(h_size, a_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # the forward pass of the network\n",
    "        x = F.relu(self.L1(x))\n",
    "        x = self.L2(x)\n",
    "        x = F.softmax(x,dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # setting the state to tensor with the input shape\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.03125834,  0.03659783,  0.0251511 , -0.00340792], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03125834,  0.03659783,  0.0251511 , -0.00340792], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.asanyarray(state[0])\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_policy = policy(s_size, 16, a_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_action, temp_prob = debug_policy.random_act(state)\n",
    "temp_action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        state = np.asanyarray(state[0])\n",
    "        \n",
    "        \n",
    "\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            \n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            \n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        \n",
    "        \n",
    "        for t in range(n_steps-1, 0, -1):\n",
    "            current_t = (returns[0] if len(returns) > 0 else 0)\n",
    "            returns.appendleft(current_t * gamma + rewards[t])\n",
    "        \n",
    "        eps = np.finfo(np.float32).eps.item() # getting the smallest positive float\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        policy_loss = []\n",
    "        \n",
    "        for action_log_prob, disconted_reward in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(- action_log_prob * disconted_reward)\n",
    "        \n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 500,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_name,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(cartpole_hyperparameters[\"env_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_policy = policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 66.07\n",
      "Episode 200\tAverage Score: 717.14\n",
      "Episode 300\tAverage Score: 984.37\n",
      "Episode 400\tAverage Score: 879.98\n",
      "Episode 500\tAverage Score: 793.53\n",
      "Episode 600\tAverage Score: 962.70\n",
      "Episode 700\tAverage Score: 1000.00\n",
      "Episode 800\tAverage Score: 985.97\n",
      "Episode 900\tAverage Score: 922.32\n",
      "Episode 1000\tAverage Score: 1000.00\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce( cartpole_policy,\n",
    "                    cartpole_optimizer,\n",
    "                    cartpole_hyperparameters[\"n_training_episodes\"], \n",
    "                    cartpole_hyperparameters[\"max_t\"],\n",
    "                    cartpole_hyperparameters[\"gamma\"], \n",
    "                    100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's obvious that i used high learning rate so i decreased it from 0.001 to 0.0001, and i will decrease the training episodes from 1000 to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters['lr'] = 1e-3\n",
    "cartpole_hyperparameters['n_training_episodes'] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = reinforce( cartpole_policy,\n",
    "                    cartpole_optimizer,\n",
    "                    cartpole_hyperparameters[\"n_training_episodes\"], \n",
    "                    cartpole_hyperparameters[\"max_t\"],\n",
    "                    cartpole_hyperparameters[\"gamma\"], \n",
    "                    100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param policy: The Reinforce agent\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in range(n_eval_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.asanyarray(state[0])\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      action, temp = policy.act(state)\n",
    "      new_state, reward, done, info, _ = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "        \n",
    "      if done:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000.0, 0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(eval_env, \n",
    "               cartpole_hyperparameters[\"max_t\"], \n",
    "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               cartpole_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'policy' object has no attribute 'act'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m cartpole_policy\u001b[39m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     action, _ \u001b[39m=\u001b[39m cartpole_policy\u001b[39m.\u001b[39;49mact(state[\u001b[39m0\u001b[39m])\n\u001b[0;32m      9\u001b[0m     state, _, done, _ , _ \u001b[39m=\u001b[39m eval_env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'policy' object has no attribute 'act'"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(\"CartPole-v1\", render_mode = 'human')\n",
    "state = eval_env.reset()\n",
    "eval_env.render()\n",
    "\n",
    "\n",
    "cartpole_policy.eval()\n",
    "while True:\n",
    "    action, _ = cartpole_policy.act(state[0])\n",
    "    state, _, done, _ , _ = eval_env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cartpole_policy.state_dict(), 'CartPole-v1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
